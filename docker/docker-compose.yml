services:
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-roast-api
    env_file:
      - .env
    volumes:
      # Mount directories that need to be shared
      - ./src:/app/src
      - ./logs:/app/logs
      - ./test_results:/app/test_results
      - ./memes:/app/memes
      - ./notebooks:/app/notebooks
      - ./datasets:/app/datasets
      - huggingface_cache:/app/.cache/huggingface
    ports:
      - "8000:8000"
    restart: unless-stopped
    environment:
      - NLTK_DATA=/root/nltk_data
      - USE_REAL_MODELS=true
      - MODEL_DEVICE=cpu
    # Add resource limits
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
    # Add healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    command: python -u run_api.py

  debug:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-roast-debug
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./test_results:/app/test_results
      - ./memes:/app/memes
      - ./notebooks:/app/notebooks
      - ./datasets:/app/datasets
      - huggingface_cache:/app/.cache/huggingface
    ports:
      - "8001:8000"  # Different port to avoid conflicts
      - "5678:5678"  # Debugpy port
    environment:
      - NLTK_DATA=/root/nltk_data
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - USE_REAL_MODELS=true
      - MODEL_DEVICE=cpu
    # Add resource limits
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
    command: python -u run_api_debug.py
    # No healthcheck in debug mode

  jupyter:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-roast-jupyter
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./test_results:/app/test_results
      - ./memes:/app/memes
      - ./notebooks:/app/notebooks
      - ./datasets:/app/datasets
      - huggingface_cache:/app/.cache/huggingface
    ports:
      - "8888:8888"
    environment:
      - NLTK_DATA=/root/nltk_data
      - USE_REAL_MODELS=true
      - MODEL_DEVICE=cpu
    # Add resource limits
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
    command: jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''

  # Simple web server for viewing HTML reports
  report-viewer:
    image: nginx:alpine
    container_name: ai-roast-report-viewer
    volumes:
      - ./test_results:/usr/share/nginx/html
      - ./memes:/usr/share/nginx/html/memes
    ports:
      - "8080:80"
    depends_on:
      - api
    restart: unless-stopped

  # Service for running improvements
  improvements:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-roast-improvements
    env_file:
      - .env
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./test_results:/app/test_results
      - ./memes:/app/memes
      - ./datasets:/app/datasets
      - huggingface_cache:/app/.cache/huggingface
    environment:
      - NLTK_DATA=/root/nltk_data
      - USE_REAL_MODELS=true
      - MODEL_DEVICE=cpu
    # Add resource limits
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2'
    command: python -m src.run_improvements --models "distilgpt2" --max-samples 5

  # OpenRouter API service
  openrouter:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: ai-roast-openrouter
    env_file:
      - .env
    volumes:
      - ../src:/app/src
      - ../scripts:/app/scripts
      - ../logs:/app/logs
      - ../test_results:/app/test_results
      - ../notebooks:/app/notebooks
      - ../docs:/app/docs
    ports:
      - "8002:8000"  # Different port to avoid conflicts with main API
    environment:
      - NLTK_DATA=/root/nltk_data
      - PYTHONPATH=/app
    # Add resource limits
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1'
    # Add healthcheck
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    command: python -m uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload

volumes:
  huggingface_cache:
    driver: local
